---
title: "Can Transformers Reason Logically? A Study in SAT Solving"
collection: publications
permalink: /publication/LLM-SAT
excerpt: 'We investigate whether decoder-only Transformers can perform logical reasoning through the lens of boolean satisfiability problems. We prove theoretically that these models can solve 3-SAT instances using backtracking and chain-of-thought prompting. We implement our construction as a PyTorch model with a tool called PARAT. Our empirical findings show that trained models generalize well to fresh problems of similar complexity but struggle with length generalization, suggesting inherent capabilities exist but with practical limitations.'
date: 2025-5-1
venue: 'Proceedings of the 42nd International Conference on Machine Learning'
paperurl: 'https://openreview.net/forum?id=5BGC2I2fxx'
citation: 'Leyan Pan, Vijay Ganesh, Jacob Abernethy, Chris Esposo, and Wenke Lee. Can transformers reason logically? A study in SAT solving. <i>Proceedings of the 42nd International Conference on Machine Learning</i>, 2025.'
---
We investigate whether decoder-only Transformers can perform logical reasoning through the lens of boolean satisfiability problems. We prove theoretically that these models can solve 3-SAT instances using backtracking and chain-of-thought prompting. We implement our construction as a PyTorch model with a tool called PARAT. Our empirical findings show that trained models generalize well to fresh problems of similar complexity but struggle with length generalization, suggesting inherent capabilities exist but with practical limitations.

[Download paper here](https://openreview.net/forum?id=5BGC2I2fxx)

Recommended citation: Leyan Pan, Vijay Ganesh, Jacob Abernethy, Chris Esposo, and Wenke Lee. Can transformers reason logically? A study in SAT solving. <i>Proceedings of the 42nd International Conference on Machine Learning</i>, 2025.
